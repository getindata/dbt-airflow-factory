"""Factory creating Airflow DAG using Astronomer Cosmos."""

import os

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from cosmos import DbtTaskGroup
from cosmos.config import RenderConfig
from cosmos.constants import LoadMode
from pytimeparse import parse

from dbt_airflow_factory.config_utils import read_config
from dbt_airflow_factory.cosmos import translate_configs
from dbt_airflow_factory.ingestion import IngestionEngine, IngestionFactory
from dbt_airflow_factory.notifications.handler import NotificationHandlersFactory


class AirflowDagFactory:
    """
    Factory creating Airflow DAG using Astronomer Cosmos for dbt task generation.

    This factory uses Cosmos under the hood for dbt task execution while reading
    configuration from standard YAML files (dbt.yml, execution_env.yml, k8s.yml, etc.).

    :param dag_path: path to directory containing config/ and manifest.json
    :type dag_path: str
    :param env: name of the environment (e.g., 'dev', 'prod')
    :type env: str
    :param dbt_config_file_name: name of the DBT config file (default: dbt.yml)
    :type dbt_config_file_name: str
    :param execution_env_config_file_name: name of the execution env config file (default: execution_env.yml)
    :type execution_env_config_file_name: str
    :param airflow_config_file_name: name of the Airflow config file (default: airflow.yml)
    :type airflow_config_file_name: str
    """

    dag_path: str
    """path to directory containing config/ and manifest.json"""
    env: str
    """name of the environment"""
    airflow_config_file_name: str
    """name of the Airflow config file (default: airflow.yml)"""

    def __init__(
        self,
        dag_path: str,
        env: str,
        dbt_config_file_name: str = "dbt.yml",
        execution_env_config_file_name: str = "execution_env.yml",
        airflow_config_file_name: str = "airflow.yml",
        airbyte_config_file_name: str = "airbyte.yml",
        ingestion_config_file_name: str = "ingestion.yml",
    ):
        self._notifications_handlers_builder = NotificationHandlersFactory()
        self.dag_path = dag_path
        self.env = env
        self.dbt_config_file_name = dbt_config_file_name
        self.execution_env_config_file_name = execution_env_config_file_name

        self.airflow_config = self._read_config(dag_path, env, airflow_config_file_name)

        airbyte_config = read_config(dag_path=dag_path, env=env, file_name=airbyte_config_file_name)
        self.ingestion_config = read_config(
            dag_path=dag_path, env=env, file_name=ingestion_config_file_name
        )
        self.ingestion_tasks_builder_factory = IngestionFactory(
            ingestion_config=airbyte_config,
            name=IngestionEngine.value_of(
                self.ingestion_config.get("engine", IngestionEngine.AIRBYTE.value)
            ),
        )

    def create(self) -> DAG:
        """
        Create Airflow DAG with dbt tasks generated by Cosmos.

        Parses manifest.json and creates tasks using Astronomer Cosmos DbtTaskGroup.
        Preserves all existing features: Airbyte ingestion, notifications, DataHub, etc.

        :return: Generated DAG with Cosmos-powered dbt tasks
        :rtype: airflow.models.dag.DAG
        """
        with DAG(
            default_args=self.airflow_config["default_args"], **self.airflow_config["dag"]
        ) as dag:
            self.create_tasks()
        return dag

    def create_tasks(self) -> None:
        """
        Create all tasks in the DAG: start, ingestion, dbt (via Cosmos), and end tasks.

        Task flow:
        1. Start dummy operator
        2. Airbyte ingestion tasks (if enabled)
        3. Cosmos DbtTaskGroup for dbt models
        4. End dummy operator
        """
        ingestion_enabled = self.ingestion_config.get("enable", False)

        # Create start task - users can reference with dag.get_task("start") to wire custom tasks
        start = EmptyOperator(task_id="start")

        ingestion_tasks = None
        if ingestion_enabled and self.ingestion_tasks_builder_factory:
            builder = self.ingestion_tasks_builder_factory.create()
            ingestion_tasks = builder.build()

        dbt_task_group = self._create_dbt_task_group()
        end = EmptyOperator(task_id="end")

        if ingestion_tasks:
            start >> ingestion_tasks >> dbt_task_group
        else:
            start >> dbt_task_group
        dbt_task_group >> end

    def _create_dbt_task_group(self) -> DbtTaskGroup:
        """
        Create Cosmos DbtTaskGroup from existing configuration files.

        Reads dbt.yml, execution_env.yml, k8s.yml, datahub.yml, and optional
        cosmos.yml, then translates them to Cosmos configuration objects.

        :return: Cosmos DbtTaskGroup containing all dbt tasks
        :rtype: cosmos.DbtTaskGroup
        """
        dbt_config = read_config(self.dag_path, self.env, self.dbt_config_file_name)
        execution_env_config = read_config(
            self.dag_path, self.env, self.execution_env_config_file_name
        )

        exec_type = execution_env_config.get("type", "k8s")
        operator_config_file = f"{exec_type}.yml"
        operator_config = read_config(
            dag_path=self.dag_path, env=self.env, file_name=operator_config_file
        )

        datahub_config = read_config(dag_path=self.dag_path, env=self.env, file_name="datahub.yml")
        cosmos_config = read_config(dag_path=self.dag_path, env=self.env, file_name="cosmos.yml")

        manifest_path = self._manifest_file_path()

        dag_id = self.airflow_config.get("dag", {}).get("dag_id")

        project_config, profile_config, execution_config, operator_args = translate_configs(
            dbt_config=dbt_config,
            execution_env_config=execution_env_config,
            k8s_config=operator_config if exec_type == "k8s" else None,
            datahub_config=datahub_config if datahub_config else None,
            cosmos_config=cosmos_config if cosmos_config else None,
            manifest_path=manifest_path,
            dag_id=dag_id,
        )

        dbt_task_group = DbtTaskGroup(
            group_id="dbt",
            project_config=project_config,
            profile_config=profile_config,
            execution_config=execution_config,
            operator_args=operator_args,
            render_config=RenderConfig(
                load_method=LoadMode.DBT_MANIFEST  # Read directly from manifest.json (fast, no dbt CLI)
            ),
        )

        return dbt_task_group

    def _manifest_file_path(self) -> str:
        """
        Get path to manifest.json file.

        :return: Absolute path to manifest.json
        :rtype: str
        """
        manifest_path = self.airflow_config.get("manifest_file_name", "manifest.json")

        # If absolute path, use as-is; otherwise join with dag_path
        if os.path.isabs(manifest_path):
            return manifest_path

        return os.path.join(self.dag_path, manifest_path)

    def _read_config(self, dag_path: str, env: str, airflow_config_file_name: str) -> dict:
        """
        Read airflow.yml from config directory into a dictionary.

        Processes retry_delay and failure_handlers for notifications.

        :return: Dictionary representing airflow.yml
        :rtype: dict
        :raises KeyError: No default_args key in airflow.yml
        """
        config = read_config(dag_path, env, airflow_config_file_name, replace_jinja=True)
        if "retry_delay" in config["default_args"]:
            config["default_args"]["retry_delay"] = parse(config["default_args"]["retry_delay"])
        if "failure_handlers" in config:
            config["default_args"][
                "on_failure_callback"
            ] = self._notifications_handlers_builder.create_failure_handler(
                config["failure_handlers"]
            )
        return config
